{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connectome Analysis\n",
    "\n",
    "Notebook accompanying Zarkali et al. Dementia risk in Parkinsonâ€™s disease is associated with interhemispheric connectivity loss and determined by regional gene expression. NeuroImage Clinical 2020\n",
    "\n",
    "Author: A Zarkali\n",
    "Last updated: 30 June 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import percentileofscore\n",
    "import statsmodels.api as sm\n",
    "#import statsmodels.formula.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import brainconn as con\n",
    "import bct as bct\n",
    "import networkx as nx\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in the clinical data\n",
    "df = pd.read_excel(r\"Data/Participants.csv\")\n",
    "participantList = df.Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define modules\n",
    "\n",
    "1) First calculate a group average connectome for all participants (can be found in the repository data folder as \"Average_Connectome.csv\")\n",
    "\n",
    "2) Run the Community Louvain algorithm (1000 iterations, gamma=1 and replicate using gamma=0.8 and gamma=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) Calculate the group average connectome \n",
    "\n",
    "array = np.zeros((379, 379))\n",
    "data_folder = Path(r\"Data\\Connectomes\")\n",
    "\n",
    "for i in range(0, len(participantList)):\n",
    "    path = data_folder / participantList[i] / \"connectome_norm.csv\"\n",
    "    data = pd.read_csv(path, header=None, sep=\" \")\n",
    "    array = array + data\n",
    "array = array / len(participantList)\n",
    "np.savetxt(\"Average_Connectome.csv\", array, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2) Run Community Louvain algorithm\n",
    "\n",
    "from functions import module_detection ## load the relevant function from the accompanying file\n",
    "gamma = 1.0 ## for replication analyses change to 0.8 and 1.3 respectively\n",
    "\n",
    "x,y = module_detection(r\"Data\\Connectomes\\Average_Connectome.csv\", gamma, 1000)\n",
    "np.savetxt(\"ModuleAllocation_gamma1.txt\", x, delimiter=\",\")\n",
    "np.savetxt(\"Louvain_vectors_gamma1.txt\", y, delimiter= \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate connectome density per participant\n",
    "\n",
    "This is already calculated and provided as a data column (\"Density\") in Data/Participants.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate connectome density for each participant\n",
    "### this has been merged on Data/Participants.csv as the Density data column\n",
    "\n",
    "## Find connectome density per connectome\n",
    "data_folder = Path(r\"Data\\Connectomes\")\n",
    "list = []\n",
    "for i in range(len(participantList)):\n",
    "    path = data_folder / participantList[i] / \"connectome_norm.csv\"\n",
    "    data = pd.read_csv(path, header=None, sep=\" \")\n",
    "    den, x, y = con.physical_connectivity.density_und(data)\n",
    "    list.append(den)\n",
    "np.savetxt(\"Density.csv\", list, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare density amongst groups\n",
    "\n",
    "data = pd.read_csv(r\"Data\\Participants.csv\")\n",
    "mean = data.Density.mean()\n",
    "r, p = stats.pearsonr(data.PD, data.Density)\n",
    "\n",
    "#### return mean density, pearson correlation coefficient and p value\n",
    "mean, r, p "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate strength and path length per connection type\n",
    "\n",
    "He we will calculate the strength and path length of different connection types:\n",
    "- Subcortical-cortical\n",
    "- Intrahemispheric\n",
    "- Interhemispheric\n",
    "- Intramodular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input module allocations\n",
    "##### For the glasser atlas our module allocations for gamma = 1.0 are found in Data/Modules/ as text files\n",
    "##### You can adjust this process for different number of modules and different atlases\n",
    "modulePath = r\"Data/Modules/\"\n",
    "\n",
    "##### Load all modules\n",
    "bgLeft = np.loadtxt((modulePath + \"BGLeft.txt\"), dtype=\"int16\")\n",
    "bgLeft[:] = [x - 1 for x in bgLeft] ## as indexing of connectomes starts at 0\n",
    "bgRight = np.loadtxt((modulePath + \"BGRight.txt\"), dtype=\"int16\")\n",
    "bgRight[:] = [x - 1 for x in bgRight]\n",
    "module1 = np.loadtxt((modulePath + \"Module1.txt\"), dtype=\"int16\")\n",
    "module1[:] = [x - 1 for x in module1]\n",
    "module1 = np.loadtxt((modulePath + \"Module1.txt\"), dtype=\"int16\")\n",
    "module1[:] = [x - 1 for x in module1]\n",
    "module2 = np.loadtxt((modulePath + \"Module2.txt\"), dtype=\"int16\")\n",
    "module2[:] = [x - 1 for x in module2]\n",
    "module3 = np.loadtxt((modulePath + \"Module3.txt\"), dtype=\"int16\")\n",
    "module3[:] = [x - 1 for x in module3]\n",
    "module4 = np.loadtxt((modulePath + \"Module4.txt\"), dtype=\"int16\")\n",
    "module4[:] = [x - 1 for x in module4]\n",
    "module5 = np.loadtxt((modulePath + \"Module5.txt\"), dtype=\"int16\")\n",
    "module5[:] = [x - 1 for x in module5]\n",
    "module6 = np.loadtxt((modulePath + \"Module6.txt\"), dtype=\"int16\")\n",
    "module6[:] = [x - 1 for x in module6]\n",
    "module7 = np.loadtxt((modulePath + \"Module7.txt\"), dtype=\"int16\")\n",
    "module7[:] = [x - 1 for x in module7]\n",
    "module8 = np.loadtxt((modulePath + \"Module8.txt\"), dtype=\"int16\")\n",
    "module8[:] = [x - 1 for x in module8]\n",
    "\n",
    "#### Input hemispheres (modules 1-4 are Left, modules 5-8 are right)\n",
    "cortexLeft = module1 + module2 + module3 + module4\n",
    "cortexRight = module5 + module6 + module7 + module8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate strength \n",
    "from functions import module_connections ## import relevant function from the accompanying functions.py \n",
    "\n",
    "sumStrength = [ ] ## empty list to hold data\n",
    "data_folder = Path(r\"Data/Connectomes\") ## root data folder\n",
    "\n",
    "# Loop through all subjects\n",
    "# Need to run this once for each connection type \n",
    "#### Example here for interhemispheric connections between module 1 and module 5\n",
    "for i in range(len(participantList)):\n",
    "    path = data_folder / participantList[i] / \"connectome_norm.csv\" \n",
    "    graph = pd.read_csv(path, header=None, sep=\" \")\n",
    "    x = module_connections(graph, module1, module5)\n",
    "    sumStrength.append(x)\n",
    "np.savetxt(\"Strength\\StrengthInter1_5.txt\", sumStrength, delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate path length\n",
    "from functions import module_length ## import relevant function from the accompanying functions.py \n",
    "\n",
    "sumStrength = [ ] ## empty list to hold data\n",
    "data_folder = Path(r\"Data/Connectomes\") ## root data folder\n",
    "\n",
    "# Loop through all subjects\n",
    "# Need to run this once for each connection type \n",
    "#### Example here for interhemispheric connections between module 1 and module 5\n",
    "for i in range(len(participantList)):\n",
    "    path = data_folder / participantList[i] / \"new_connectome.csv\" ### need the length connectomes\n",
    "    graph = pd.read_csv(path, header=None, sep=\" \")\n",
    "    x = module_length(graph, module1 module5)\n",
    "    sumStrength.append(x)\n",
    "np.savetxt(\"Length\\LengthInter1_5.txt\", sumStrength, delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge files to a single df\n",
    "#### Example to merge all txt files from the calculated path length and strenght to a single csv file\n",
    "import os\n",
    "import glob\n",
    "os.chdir(r\"Strength/\") ### This is the directory that holds all the txt files you want to merge\n",
    "extension = 'txt'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=None) for f in all_filenames], axis=0)\n",
    "#export to csv\n",
    "combined_csv.to_csv(\"Strength_Combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Strength and Length combined csv files can be used for between-group comparisons in combination with the Participants.csv.\n",
    "\n",
    "To do this we used statsmodels' Mixed Linear model (MixedLM.from_formula) with age and gender as covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate atrophy rates\n",
    "\n",
    "Here we will transform the connection strength values for each participant to atrophy rates, compared to controls. \n",
    "\n",
    "First we will calculate a z score for each connection.\n",
    "\n",
    "Then we will tahn transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate atrophy rates compared to controls\n",
    "## Load the average control connectome\n",
    "average_path = Path(r\"Data\\Connectomes\\Average_Connectome.csv\") ## the mean strength per connection for the group-averaged control connectome\n",
    "std_path = Path(r\"Data\\Connectomes\\STD_Connectome.csv\") ## the standard deviation per connection for the group-averaged control connectome\n",
    "average_connectome = np.genfromtxt(average_path, delimiter=\",\")\n",
    "std_connectome = np.genfromtxt(std_path, delimiter=\",\")\n",
    "\n",
    "data_folder = Path(r\"Data\\Connectomes\")\n",
    "for i in range(len(participantList)):\n",
    "    path = data_folder / participantList[i] / \"connectome_norm.csv\"\n",
    "    ## Load connectomes\n",
    "    data = np.genfromtxt(path)\n",
    "    # Calculate z score\n",
    "    data_z = (average_connectome - data) / std_connectome\n",
    "    export_name = str(participantList[i]) + \"_Zscore.txt\"\n",
    "    np.savetxt(export_name, data_z, delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tahn transform for each participant\n",
    "data_folder = Path(r\"Data\\Connectomes\")\n",
    "average_z = np.zeros((379,379)) # create an empty array to hold results\n",
    "pdList = df[df.PD == 1].Participants # load a list of PD participants only\n",
    "## Loop through all PD participants and perform tahn transform\n",
    "for i in range(len(pdList)):\n",
    "    path = data_folder / str(pdList[i] + \"_Zscore.txt\")\n",
    "    data = np.genfromtxt(path)\n",
    "    transform = np.tanh(data)\n",
    "    export_name = str(pdList[i] + \"_tahn.txt\")\n",
    "    np.savetxt(export_name, transform, delimiter=\" \")\n",
    "## These can then be grouped to calculate average tahn transformed values across different groups (ie PD vs controls etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
